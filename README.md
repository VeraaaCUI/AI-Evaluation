# A Comprehensive Overview of Evaluation Methods of Large Models

## General Evaluations
- **MMLU** [[1]](https://arxiv.org/abs/2009.03300): Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt. "Measuring massive multitask language understanding." arXiv preprint arXiv:2009.03300 (2020).
- **DynaBench** [[2]](https://arxiv.org/abs/2104.14337): Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. "DynaBench: Rethinking benchmarking in NLP." arXiv preprint arXiv:2104.14337 (2021).
- **MT-Bench** [[3]](https://arxiv.org/abs/2211.09110): Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. "Holistic evaluation of language models." arXiv preprint arXiv:2211.09110 (2022).
- **BIG-bench** [[4]](https://arxiv.org/abs/2206.04615): Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri√† Garriga-Alonso, et al. "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models." arXiv preprint arXiv:2206.04615 (2022).
- **GLUE-X** [[5]](https://arxiv.org/abs/2211.08073): Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. "Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective." arXiv preprint arXiv:2211.08073 (2022).
- **Xiezhi** [[6]](https://arxiv.org/abs/2306.05783): Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, et al. "Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge Evaluation." arXiv preprint arXiv:2306.05783 (2023).
- **C-Eval** [[7]](https://arxiv.org/abs/2305.08322): Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. "C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models." arXiv preprint arXiv:2305.08322 (2023).
- **OpenLLM** [[8]](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard): HuggingFace. "Open-source Large Language Models Leaderboard." (2023).
- **Chatbot Arena** [[9]](https://lmsys.org): LMSYS. "Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings." (2023).
- **AlpacaEval** [[10]](https://github.com/tatsu-lab/alpaca_eval): Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. "AlpacaEval: An Automatic Evaluator of Instruction-following Models." (2023).
- **PandaLM** [[11]](https://arxiv.org/abs/2306.05087): Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie,

